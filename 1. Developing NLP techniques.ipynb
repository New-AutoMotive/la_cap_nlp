{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('language': conda)",
   "metadata": {
    "interpreter": {
     "hash": "bef506280397be3d8a1ecb78e023b21807d0c18ad22fc0e7adc4283ecda49171"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NLP and Local Authority Climate Action Plans"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The plan is to programmatically assess how Local Authority climate action plans score for their treatment of EVs. This will involve several stages. \n",
    "\n",
    "1. Importing the LA Climate Action plans\n",
    "2. Making them machine readable and ready for NLP\n",
    "3. NLP to find the parts that are relevant to electric vehicles and charging infrastructure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Packages go here:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading & pre-processing tools\n",
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import requests\n",
    "import re\n",
    "\n",
    "#NLP\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Importing a Climate Action Plan\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Helpfully, MySociety have created a list of local authority climate action plans here (https://docs.google.com/spreadsheets/d/1tEnjJRaWsdXtCkMwA25-ZZ8D75zAY6c2GOOeUchZsnU/edit#gid=0). \n",
    "\n",
    "We're going to start with Bristol. \n",
    "\n",
    "In time we will automatically read URLs from the MySociety Document, but until then, we'll do an old fashioned copy-paste. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Download the file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_url = 'https://www.bristolonecity.com/wp-content/uploads/2020/02/one-city-climate-strategy.pdf'\n",
    "r = requests.get(cs_url)\n",
    "with open('bristol.pdf', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "source": [
    "### Create a 'Bristol' folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('Bristol'):\n",
    "    os.mkdir('Bristol')"
   ]
  },
  {
   "source": [
    "### Scrape the text from the PDF, clean and tidy\n",
    "Annoyingly, it is not in a very machine-readable format. Will need some clean up. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = PyPDF2.PdfFileReader(open('bristol.pdf', 'rb'))\n",
    "for i in range(pdf.numPages): \n",
    "#might be able to optimise this later - it currently scrapes off each page separately - probably an easier way to get it into one single document\n",
    "    num = str(i)\n",
    "    with open(f'Bristol/bristol{num}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(pdf.pages[i].extractText())"
   ]
  },
  {
   "source": [
    "Now to put it into one file: bristol_full.txt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('Bristol/bristol*.txt')\n",
    "\n",
    "#create an empty text file to receive the text...\n",
    "with open('Bristol/full_text.txt', 'w', encoding = 'utf-8') as f:\n",
    "    f.write(' ')\n",
    "    pass #leave it blank for now - will loop through the data and append it here later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect the data all into the one file, deleting empty lines as we go along, and removing line breaks. \n",
    "if not os.path.isfile('Bristol/bristol_full.txt'):\n",
    "    for i in range(len(files)):\n",
    "        item = files[i]\n",
    "        with open(item, 'r+', encoding='utf-8') as f:\n",
    "            text = f.readlines()\n",
    "            #print(text)\n",
    "            new_text = ''\n",
    "            for line in text:\n",
    "                new_text += line.replace('\\n', '')\n",
    "            # now append text to master text file\n",
    "            with open('Bristol/bristol_full.txt', 'a', encoding='utf-8') as p:\n",
    "                p.write(new_text)\n",
    "else:\n",
    "    print(\"There's already a document called 'bristol_full.txt', delete this first if you want to create a fresh one.\")\n"
   ]
  },
  {
   "source": [
    "## Cleaning up the document\n",
    "Everything's a bit messy. Lots of whitespace loss, odd additional page numbers. We'll clean it up now.\n",
    "\n",
    "First, inserting spaces between digits followed immediately by letters, or letters followed immediately by digits. E.g. \"In2030 Bristol will\" or \"Net Zero by2050\". "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "144871\n"
     ]
    }
   ],
   "source": [
    "#Open the document and save the text to climate_plan_text\n",
    "climate_plan_text = ''\n",
    "with open('Bristol/bristol_full.txt', 'r', encoding='utf-8') as f:\n",
    "    climate_plan_text = f.read()\n",
    "\n",
    "print(len(climate_plan_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a regex expression to recognise letters followed by digits or digits followed by letters\n",
    "dl = re.compile(r'(?<=\\d)(?=[^\\d\\s,.])|(?<=[^\\d\\s,.])(?=\\d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bristol will be carbon neutral by 2030 and from 2040 will only use 140 EVs. 2025 will see the launch of 1.5 kWh battery.\n"
     ]
    }
   ],
   "source": [
    "# Run a test\n",
    "test_string = 'Bristol will be carbon neutral by2030 and from 2040will only use 140EVs. 2025 will see the launch of 1.5kWh battery.'\n",
    "print(dl.sub(' ', test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the changes\n",
    "climate_plan_text += dl.sub(' ', climate_plan_text)"
   ]
  },
  {
   "source": [
    "In some places, years are followed by an extra digit, usually a rogue page number, or something like that. Years are important because they indicate the timing, and ambition, of particular commitments, so we'd like to preserve years where possible. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regex that matches the 5th digit in a 5 digit number. We assume that any other numbers use comma separations. \n",
    "yr = re.compile(r'(\\d)(?<=\\d{5})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "climate resilient Bristol by 2030 Foreword\n"
     ]
    }
   ],
   "source": [
    "test_string_2 = 'climate resilient Bristol by 20302 Foreword'\n",
    "print(yr.sub('', test_string_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply this to the document:\n",
    "climate_plan_text = yr.sub('', climate_plan_text)"
   ]
  },
  {
   "source": [
    "We have some full stops that do not have spaces after them, too. Now to clear up those."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a regex expression to match the full stops surrounded by letters and insert a space.\n",
    "fs = re.compile(r'\\.(?=\\w)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "in Bristol. This will help. Testing, testing. Some are. Written correctly.\n"
     ]
    }
   ],
   "source": [
    "#Run a test...\n",
    "test_string_3 = 'in Bristol.This will help.Testing, testing. Some are. Written correctly.'\n",
    "print(fs.sub('. ', test_string_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply to text\n",
    "climate_plan_text = fs.sub('. ', climate_plan_text)"
   ]
  },
  {
   "source": [
    "Bristol/bristol_full.txt is now a pretty good text rendering of Bristol's climate action plan. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Extracting keyword sentences.\n",
    "Using SpaCy to extract keywords and do other document analysis. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = climate_plan_text #use the raw text - not tokenized or processed beyond being a clean(ish) txt file\n",
    "\n",
    "phrases  = ['electric vehicles', 'electric vehicle', 'charging', 'cars', 'zero emission vehicles', 'emission vehicles']\n",
    "\n",
    "patterns = [nlp(t) for t in phrases]\n",
    "\n",
    "matcher.add('EVs', None, *patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#list for some tags:\n",
    "tags = []\n",
    "\n",
    "#list for some sentences:\n",
    "tagged_sents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "charging\ncharging\nelectric vehicles\nelectric vehicle\ncharging\ncars\ncars\ncharging\ncars\nemission vehicles\nelectric vehicle\ncharging\ncharging\nemission vehicles\nemission vehicles\ncharging\ncharging\nelectric vehicles\nelectric vehicle\ncharging\ncars\ncars\ncharging\ncars\nemission vehicles\nelectric vehicle\ncharging\ncharging\nemission vehicles\nemission vehicles\n"
     ]
    }
   ],
   "source": [
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    tags.append(span.text)\n",
    "    tagged_sents.append(span.sent)\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                tags                                       tagged_sents\n",
       "0           charging  (This, will, include, working, with, regulator...\n",
       "1           charging  (We, will, need, signi˜cant, new, walking, ,, ...\n",
       "2  electric vehicles  (We, will, need, signi˜cant, new, walking, ,, ...\n",
       "3   electric vehicle  ( , -Development, of, a, citywide, plan, for, ...\n",
       "4           charging  ( , -Development, of, a, citywide, plan, for, ..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tags</th>\n      <th>tagged_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>charging</td>\n      <td>(This, will, include, working, with, regulator...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>charging</td>\n      <td>(We, will, need, signi˜cant, new, walking, ,, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>electric vehicles</td>\n      <td>(We, will, need, signi˜cant, new, walking, ,, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>electric vehicle</td>\n      <td>( , -Development, of, a, citywide, plan, for, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>charging</td>\n      <td>( , -Development, of, a, citywide, plan, for, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "data = {'tags': tags, 'tagged_sents': tagged_sents}\n",
    "df = pd.DataFrame(data = data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}